{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68045cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" In this notebook, we will look into only one specialist model. \"\"\"\n",
    "\n",
    "# Load libraries \n",
    "import torch \n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "import torch.optim as optim \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import random \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45bc17df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_specialist_dataset(dataset, subset_class, transformation, batch_size, train, dataloader, shuffle = True): \n",
    "    \"\"\" Returns dataset enriched with examples that the specialist model specializes in. \"\"\"\n",
    "    \n",
    "    # Gather all images that belong to specialist's sub-class \n",
    "    num_examples = len(dataset.targets)\n",
    "    \n",
    "    subset_indices = [] \n",
    "    for label in subset_class: \n",
    "        indices = [i for i in range(num_examples) if dataset.targets[i] == label]\n",
    "        subset_indices.append(indices)\n",
    "        \n",
    "    # Flatten the list of lists into one list \n",
    "    subset_indices = [item for sublist in subset_indices for item in sublist]\n",
    "    \n",
    "    # Get training data from dustbin class\n",
    "    if train: \n",
    "        num_dustbin_examples = 500\n",
    "    else: \n",
    "        num_dustbin_examples = 100 \n",
    "        \n",
    "    random_indices = np.random.randint(0, num_examples - 1, num_dustbin_examples * 5)\n",
    "    dustbin_indices = [] \n",
    "    for index in random_indices: \n",
    "        if index in subset_indices: \n",
    "            continue\n",
    "        else: \n",
    "            dustbin_indices.append(index)          \n",
    "    random.shuffle(dustbin_indices)\n",
    "    dustbin_indices = dustbin_indices[:num_dustbin_examples]\n",
    "    \n",
    "    # Combine examples from specialised subset and dustbin class \n",
    "    indices = subset_indices + dustbin_indices \n",
    "    \n",
    "    # Create dataset \n",
    "    specialist_dataset, specialist_dataset_targets = [dataset.data[i] for i in indices], [dataset.targets[i] for i in indices]\n",
    "    torch_specialist_dataset = specialistDataset(specialist_dataset, specialist_dataset_targets, transform = transformation)\n",
    "    if not dataloader: \n",
    "        return torch_specialist_dataset\n",
    "    \n",
    "    specialist_dataloader = torch.utils.data.DataLoader(torch_specialist_dataset, batch_size = batch_size, shuffle = shuffle)\n",
    "    return specialist_dataloader\n",
    "\n",
    "\n",
    "class specialistDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        super(specialistDataset, self).__init__()\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.images[idx]\n",
    "        if self.transform: \n",
    "            sample = self.transform(sample)\n",
    "        label = self.labels[idx]\n",
    "        return sample, label \n",
    "\n",
    "    \n",
    "def train_and_evaluate_specialist(trainloader, testloader, specialist_model, optimizer, scheduler, criterion, num_epochs, domain, model_path): \n",
    "    \n",
    "    lowest_test_loss = 10.0\n",
    "    dustbin_class = len(domain)\n",
    "    specialist_model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs): \n",
    "        running_loss, corrects = 0.0, 0 \n",
    "        train_total = 0 \n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0): \n",
    "            inputs, labels = data \n",
    "            \n",
    "            # Correct the labels\n",
    "            for index in range(labels.shape[0]): \n",
    "                label = labels[index]\n",
    "                if label in domain: \n",
    "                    labels[index] = domain.index(label)\n",
    "                else: \n",
    "                    labels[index] = dustbin_class\n",
    "                \n",
    "            if torch.cuda.is_available(): \n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                \n",
    "            # Zero the parameter gradients \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward + Backward + Optimise\n",
    "            outputs = specialist_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate statistics\n",
    "            running_loss += loss.item()\n",
    "            predicted_class = outputs.data.max(1, keepdim = True)[1]\n",
    "            corrects += predicted_class.eq(labels.data.view_as(predicted_class)).cpu().sum()\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "        # Evaluation \n",
    "        test_correct, test_total = 0, 0 \n",
    "        test_running_loss = 0.0\n",
    "        with torch.no_grad(): \n",
    "            for data in testloader: \n",
    "                images, labels = data \n",
    "                \n",
    "                for j in range(labels.shape[0]): \n",
    "                    label = labels[j]\n",
    "                    if label in domain: \n",
    "                        labels[j] = domain.index(label)\n",
    "                    else: \n",
    "                        labels[j] = dustbin_class              \n",
    "                \n",
    "                if torch.cuda.is_available(): \n",
    "                    images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "                # Forward + Backward + Optimise\n",
    "                outputs = specialist_model(images)\n",
    "                test_loss = criterion(outputs, labels)\n",
    "            \n",
    "                # Calculate testing statistics\n",
    "                test_running_loss += test_loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).cpu().sum().item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if test_running_loss/test_total < lowest_test_loss: \n",
    "            torch.save(specialist_model.state_dict(), model_path)\n",
    "            lowest_test_loss = test_running_loss/test_total\n",
    "    \n",
    "        print(f'[{epoch + 1}] train_loss: {running_loss/train_total}, test_loss: {test_running_loss/test_total}, train_acc: {corrects/train_total}, test_acc: {test_correct/test_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a97d8133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_specialist(model, testloader, domain):\n",
    "    test_correct, test_total = 0, 0 \n",
    "    dustbin_class = len(domain)\n",
    "    with torch.no_grad(): \n",
    "        for data in testloader: \n",
    "            images, labels = data \n",
    "            \n",
    "            # Correct the labels \n",
    "            for j in range(labels.shape[0]): \n",
    "                label = labels[j]\n",
    "                if label in domain: \n",
    "                    labels[j] = domain.index(label)\n",
    "                else: \n",
    "                    labels[j] = dustbin_class              \n",
    "                \n",
    "            if torch.cuda.is_available(): \n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # Calculate statistics \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).cpu().sum().item()\n",
    "            \n",
    "    return test_correct*100/test_total\n",
    "\n",
    "\n",
    "def evaluate(model, testloader): \n",
    "    test_correct, test_total = 0, 0 \n",
    "    with torch.no_grad(): \n",
    "        for data in testloader: \n",
    "            images, labels = data \n",
    "            if torch.cuda.is_available():\n",
    "                images, labels = images.cuda(), labels.cuda() \n",
    "      \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).cpu().sum().item() \n",
    "      \n",
    "    return test_correct*100/test_total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0e057c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load and normalise data \n",
    "transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
    "\n",
    "train_batch_size, test_batch_size = 128, 1024\n",
    "trainset = torchvision.datasets.CIFAR100(root = './data', train = True, download = True, transform = transformation)\n",
    "testset = torchvision.datasets.CIFAR100(root = './data', train = False, download = True, transform = transformation)\n",
    "\n",
    "num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b03ef2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\yeewenli/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] train_loss: 0.01913579797744751, test_loss: 0.013277672350406647, train_acc: 0.3292999863624573, test_acc: 0.5875\n",
      "[2] train_loss: 0.009995295268297195, test_loss: 0.0092293221950531, train_acc: 0.7045999765396118, test_acc: 0.6815\n",
      "[3] train_loss: 0.007291124790906906, test_loss: 0.007724655032157898, train_acc: 0.7764000296592712, test_acc: 0.7255\n",
      "[4] train_loss: 0.006032058668136596, test_loss: 0.006898874849081039, train_acc: 0.8123999834060669, test_acc: 0.7395\n",
      "[5] train_loss: 0.0051697238743305205, test_loss: 0.0064776366353034975, train_acc: 0.8343999981880188, test_acc: 0.757\n",
      "[6] train_loss: 0.004619477725028992, test_loss: 0.006115018844604492, train_acc: 0.8529999852180481, test_acc: 0.764\n",
      "[7] train_loss: 0.004149292653799057, test_loss: 0.005906160563230515, train_acc: 0.8657000064849854, test_acc: 0.762\n",
      "[8] train_loss: 0.003807839462161064, test_loss: 0.005649289056658745, train_acc: 0.8759999871253967, test_acc: 0.774\n",
      "[9] train_loss: 0.0034924988836050035, test_loss: 0.00555022719502449, train_acc: 0.8865000009536743, test_acc: 0.7765\n",
      "[10] train_loss: 0.0031776144862174986, test_loss: 0.005515244424343109, train_acc: 0.8996999859809875, test_acc: 0.781\n",
      "[11] train_loss: 0.0029678739354014397, test_loss: 0.005459706485271454, train_acc: 0.9075999855995178, test_acc: 0.779\n",
      "[12] train_loss: 0.0027274646744132043, test_loss: 0.0053624527156352995, train_acc: 0.9154999852180481, test_acc: 0.7855\n",
      "[13] train_loss: 0.002621836943924427, test_loss: 0.005365579426288605, train_acc: 0.9192000031471252, test_acc: 0.781\n",
      "[14] train_loss: 0.002382800726592541, test_loss: 0.005268334925174713, train_acc: 0.9265999794006348, test_acc: 0.788\n",
      "[15] train_loss: 0.0022217004507780074, test_loss: 0.00534045872092247, train_acc: 0.9333000183105469, test_acc: 0.7855\n",
      "[16] train_loss: 0.002113657769560814, test_loss: 0.005323124676942825, train_acc: 0.9409000277519226, test_acc: 0.7855\n",
      "[17] train_loss: 0.00198809564858675, test_loss: 0.0052754213660955425, train_acc: 0.9477999806404114, test_acc: 0.788\n",
      "[18] train_loss: 0.0018497582152485848, test_loss: 0.005382715135812759, train_acc: 0.9517999887466431, test_acc: 0.787\n",
      "[19] train_loss: 0.0017042876362800597, test_loss: 0.005287827014923096, train_acc: 0.9556999802589417, test_acc: 0.7905\n",
      "[20] train_loss: 0.001625301867723465, test_loss: 0.005324772298336029, train_acc: 0.9603000283241272, test_acc: 0.7895\n"
     ]
    }
   ],
   "source": [
    "sub_class = [12, 17, 23, 33, 37, 47, 49, 52, 56, 59, 60, 68, 69, 71, 76, 81, 85, 90, 96]\n",
    "\n",
    "specialist_model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_resnet20\", pretrained = False)\n",
    "generalist_state_dict = torch.load(\"teacher_model_cifar100.pth\")\n",
    "specialist_model.load_state_dict(generalist_state_dict)\n",
    "specialist_model.fc = nn.Linear(specialist_model.fc.in_features, len(sub_class) + 1)   # +1 for dustbin class\n",
    "if torch.cuda.is_available(): \n",
    "    specialist_model = specialist_model.cuda() \n",
    "    \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(specialist_model.parameters(), lr = 0.001, nesterov = True, momentum = 0.9, weight_decay = 5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.1)\n",
    "num_epochs = 20 \n",
    "model_path = 'specialist_0_debug.pth'\n",
    "\n",
    "specialist_train_batch_size, specialist_test_batch_size = 128, 128\n",
    "\n",
    "# Returns dataloader\n",
    "specialist_traindataloader = create_specialist_dataset(trainset, sub_class, transformation, specialist_train_batch_size, train = True, dataloader = True, shuffle = True)\n",
    "\n",
    "# Returns dataset\n",
    "specialist_testset = create_specialist_dataset(testset, sub_class, transformation, specialist_test_batch_size, train = False, dataloader = False)                               \n",
    "specialist_testdataloader = torch.utils.data.DataLoader(specialist_testset, batch_size = specialist_test_batch_size, shuffle = True)\n",
    "\n",
    "train_and_evaluate_specialist(specialist_traindataloader, specialist_testdataloader, specialist_model, optimizer, scheduler, criterion, num_epochs, sub_class, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07ce0d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle: True, batch_size: 16, testing accuracy of specialist model: 72.8 %\n",
      "shuffle: True, batch_size: 32, testing accuracy of specialist model: 75.5 %\n",
      "shuffle: True, batch_size: 64, testing accuracy of specialist model: 78.2 %\n",
      "shuffle: True, batch_size: 128, testing accuracy of specialist model: 78.85 %\n",
      "shuffle: False, batch_size: 16, testing accuracy of specialist model: 9.3 %\n",
      "shuffle: False, batch_size: 32, testing accuracy of specialist model: 10.5 %\n",
      "shuffle: False, batch_size: 64, testing accuracy of specialist model: 15.75 %\n",
      "shuffle: False, batch_size: 128, testing accuracy of specialist model: 22.0 %\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [16, 32, 64, 128]\n",
    "shuffle_boolean = [True, False]\n",
    "for i in range(len(shuffle_boolean)): \n",
    "    for size in batch_sizes: \n",
    "        specialist_testdataloader = torch.utils.data.DataLoader(specialist_testset, batch_size = size, shuffle = shuffle_boolean[i])\n",
    "        print(\"shuffle: {}, batch_size: {}, testing accuracy of specialist model: {} %\".format(shuffle_boolean[i], size, evaluate_specialist(specialist_model, specialist_testdataloader, sub_class)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
