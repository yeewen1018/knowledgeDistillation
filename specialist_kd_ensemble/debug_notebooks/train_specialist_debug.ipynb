{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68045cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" In this notebook, we will look into only one specialist model. \"\"\"\n",
    "\n",
    "# Load libraries \n",
    "import torch \n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "import torch.optim as optim \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import random \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7b668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_specialist_dataset(dataset, subset_class, batch_size, train, dataloader, shuffle = True): \n",
    "    \"\"\" Returns dataset enriched with examples that the specialist model specializes in. \"\"\"\n",
    "    \n",
    "    # Gather all images that belong to specialist's sub-class \n",
    "    num_examples = len(dataset.targets)\n",
    "    subset_indices = [] \n",
    "    for label in subset_class: \n",
    "        indices = [i for i in range(num_examples) if dataset.targets[i] == label]\n",
    "        subset_indices.append(indices)\n",
    "        \n",
    "    # Flatten the list of lists into one list \n",
    "    subset_indices = [item for sublist in subset_indices for item in sublist]\n",
    "    \n",
    "    # Get training data from dustbin class\n",
    "    if train: \n",
    "        num_dustbin_examples = 500\n",
    "    else: \n",
    "        num_dustbin_examples = 100 \n",
    "        \n",
    "    random_indices = np.random.randint(0, num_examples - 1, num_dustbin_examples * 5)\n",
    "    dustbin_indices = [] \n",
    "    for index in random_indices: \n",
    "        if index in subset_indices: \n",
    "            continue\n",
    "        else: \n",
    "            dustbin_indices.append(index)          \n",
    "    random.shuffle(dustbin_indices)\n",
    "    dustbin_indices = dustbin_indices[:num_dustbin_examples]\n",
    "    \n",
    "    # Combine examples from specialised subset and dustbin class \n",
    "    indices = subset_indices + dustbin_indices \n",
    "    \n",
    "    # Create dataset \n",
    "    specialist_dataset, specialist_dataset_targets = [dataset.data[i] for i in indices], [dataset.targets[i] for i in indices] \n",
    "    transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
    "    torch_specialist_dataset = specialistDataset(specialist_dataset, specialist_dataset_targets, subset_class, transform = transformation)\n",
    "    if not dataloader: \n",
    "        return torch_specialist_dataset\n",
    "    specialist_dataloader = torch.utils.data.DataLoader(torch_specialist_dataset, batch_size = batch_size, shuffle = shuffle)\n",
    "    return specialist_dataloader\n",
    "\n",
    "\n",
    "class specialistDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, sub_class, transform=None):\n",
    "        super(specialistDataset, self).__init__()\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.sub_class = sub_class\n",
    "        self.dustbin_class = len(self.sub_class)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.images[idx]\n",
    "        if self.transform: \n",
    "            sample = self.transform(sample)\n",
    "        label = self.labels[idx]\n",
    "        if label in self.sub_class: \n",
    "            label = self.sub_class.index(label)\n",
    "        else: \n",
    "            label = self.dustbin_class\n",
    "        return sample, label \n",
    "    \n",
    "def train_and_evaluate_scratch(trainloader, testloader, model, optimizer, scheduler, criterion, num_epochs, model_path, device): \n",
    "    lowest_test_loss = 1000.0 \n",
    "    for epoch in range(num_epochs): \n",
    "        running_loss, train_corrects, train_total = 0.0, 0, 0\n",
    "        model.train() \n",
    "        for inputs, labels in trainloader: \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            predicted_class = outputs.data.max(1, keepdim = True)[1]\n",
    "            train_corrects += predicted_class.eq(labels.data.view_as(predicted_class)).cpu().sum()\n",
    "            train_total += labels.size(0)\n",
    "    \n",
    "        # Evaluation \n",
    "        test_corrects, test_total, test_running_loss = evaluate(model, testloader, device)\n",
    "\n",
    "        scheduler.step()\n",
    "        if test_running_loss/test_total < lowest_test_loss: \n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            lowest_test_loss = test_running_loss/test_total\n",
    " \n",
    "        print(f'[{epoch + 1}], train_loss: {running_loss/train_total:.4f}, test_loss: {test_running_loss/test_total:.4f}, train_accuracy: {train_corrects*100/train_total:.2f} %, test_accuracy: {test_corrects*100/test_total:.2f} %')\n",
    "\n",
    "def evaluate(model, testloader, device):   \n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    test_running_loss, test_corrects, test_total = 0.0, 0, 0 \n",
    "    model.eval() \n",
    "    with torch.no_grad(): \n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device) \n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_running_loss += loss.item() \n",
    "            predicted_class = outputs.data.max(1, keepdim = True)[1]\n",
    "            test_corrects += predicted_class.eq(labels.data.view_as(predicted_class)).cpu().sum()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    return test_corrects, test_total, test_running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e057c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load and normalise data \n",
    "transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
    "\n",
    "train_batch_size, test_batch_size = 128, 1024\n",
    "trainset = torchvision.datasets.CIFAR100(root = './data', train = True, download = True, transform = transformation)\n",
    "testset = torchvision.datasets.CIFAR100(root = './data', train = False, download = True, transform = transformation)\n",
    "\n",
    "num_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b03ef2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\yeewenli/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1], train_loss: 0.0449, test_loss: 0.0067, train_accuracy: 63.83 %, test_accuracy: 76.10 %\n",
      "[2], train_loss: 0.0207, test_loss: 0.0056, train_accuracy: 82.40 %, test_accuracy: 78.90 %\n",
      "[3], train_loss: 0.0157, test_loss: 0.0051, train_accuracy: 85.99 %, test_accuracy: 80.85 %\n",
      "[4], train_loss: 0.0128, test_loss: 0.0051, train_accuracy: 88.63 %, test_accuracy: 80.25 %\n",
      "[5], train_loss: 0.0111, test_loss: 0.0050, train_accuracy: 90.14 %, test_accuracy: 80.40 %\n",
      "[6], train_loss: 0.0093, test_loss: 0.0051, train_accuracy: 91.94 %, test_accuracy: 80.30 %\n",
      "[7], train_loss: 0.0082, test_loss: 0.0050, train_accuracy: 92.93 %, test_accuracy: 80.80 %\n",
      "[8], train_loss: 0.0070, test_loss: 0.0051, train_accuracy: 94.25 %, test_accuracy: 80.90 %\n",
      "[9], train_loss: 0.0060, test_loss: 0.0054, train_accuracy: 95.38 %, test_accuracy: 80.70 %\n",
      "[10], train_loss: 0.0052, test_loss: 0.0054, train_accuracy: 96.02 %, test_accuracy: 80.80 %\n",
      "[11], train_loss: 0.0046, test_loss: 0.0054, train_accuracy: 96.67 %, test_accuracy: 80.95 %\n",
      "[12], train_loss: 0.0041, test_loss: 0.0056, train_accuracy: 97.11 %, test_accuracy: 80.40 %\n",
      "[13], train_loss: 0.0036, test_loss: 0.0056, train_accuracy: 97.58 %, test_accuracy: 80.40 %\n",
      "[14], train_loss: 0.0030, test_loss: 0.0056, train_accuracy: 98.21 %, test_accuracy: 80.60 %\n",
      "[15], train_loss: 0.0027, test_loss: 0.0057, train_accuracy: 98.47 %, test_accuracy: 80.00 %\n",
      "[16], train_loss: 0.0022, test_loss: 0.0059, train_accuracy: 98.98 %, test_accuracy: 81.15 %\n",
      "[17], train_loss: 0.0020, test_loss: 0.0058, train_accuracy: 99.05 %, test_accuracy: 80.20 %\n",
      "[18], train_loss: 0.0019, test_loss: 0.0060, train_accuracy: 99.06 %, test_accuracy: 80.00 %\n",
      "[19], train_loss: 0.0018, test_loss: 0.0060, train_accuracy: 99.22 %, test_accuracy: 79.95 %\n",
      "[20], train_loss: 0.0016, test_loss: 0.0060, train_accuracy: 99.31 %, test_accuracy: 80.20 %\n"
     ]
    }
   ],
   "source": [
    "sub_class = [12, 17, 23, 33, 37, 47, 49, 52, 56, 59, 60, 68, 69, 71, 76, 81, 85, 90, 96]\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "specialist_model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_resnet20\", pretrained = False)\n",
    "generalist_state_dict = torch.load(\"teacher_model_cifar100.pth\")\n",
    "specialist_model.load_state_dict(generalist_state_dict)\n",
    "specialist_model.fc = nn.Linear(specialist_model.fc.in_features, len(sub_class) + 1)   # +1 for dustbin class\n",
    "specialist_model = specialist_model.to(device)\n",
    "  \n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(specialist_model.parameters(), lr = 0.001, nesterov = True, momentum = 0.9, weight_decay = 5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.1)\n",
    "num_epochs = 20 \n",
    "model_path = 'specialist_0_debug.pth'\n",
    "\n",
    "specialist_train_batch_size, specialist_test_batch_size = 32, 128\n",
    "\n",
    "# Returns dataloader\n",
    "specialist_traindataloader = create_specialist_dataset(trainset, sub_class, specialist_train_batch_size, train = True, dataloader = True, shuffle = True)\n",
    "specialist_testset = create_specialist_dataset(testset, sub_class, specialist_test_batch_size, train = False, dataloader = False, shuffle = False)\n",
    "specialist_testdataloader = torch.utils.data.DataLoader(specialist_testset, batch_size = specialist_test_batch_size, shuffle = False)\n",
    "train_and_evaluate_scratch(specialist_traindataloader, specialist_testdataloader, specialist_model, optimizer, scheduler, criterion, num_epochs, model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ce0d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle: True, batch_size: 1, testing accuracy of specialist model 1 is 80.25%\n",
      "shuffle: True, batch_size: 16, testing accuracy of specialist model 1 is 80.25%\n",
      "shuffle: True, batch_size: 32, testing accuracy of specialist model 1 is 80.25%\n",
      "shuffle: True, batch_size: 64, testing accuracy of specialist model 1 is 80.25%\n",
      "shuffle: True, batch_size: 128, testing accuracy of specialist model 1 is 80.25%\n",
      "shuffle: False, batch_size: 1, testing accuracy of specialist model 2 is 80.25%\n",
      "shuffle: False, batch_size: 16, testing accuracy of specialist model 2 is 80.25%\n",
      "shuffle: False, batch_size: 32, testing accuracy of specialist model 2 is 80.25%\n",
      "shuffle: False, batch_size: 64, testing accuracy of specialist model 2 is 80.25%\n",
      "shuffle: False, batch_size: 128, testing accuracy of specialist model 2 is 80.25%\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [1, 16, 32, 64, 128]\n",
    "shuffle_boolean = [True, False]\n",
    "for i in range(len(shuffle_boolean)): \n",
    "    for size in batch_sizes: \n",
    "        specialist_testdataloader = torch.utils.data.DataLoader(specialist_testset, batch_size = size, shuffle = shuffle_boolean[i])\n",
    "        test_corrects, test_total, _ = evaluate(specialist_model, specialist_testdataloader, device)\n",
    "        print(f'shuffle: {shuffle_boolean[i]}, batch_size: {size}, testing accuracy of specialist model {i+1} is {test_corrects*100/test_total:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
